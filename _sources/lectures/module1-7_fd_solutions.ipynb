{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d797217",
   "metadata": {},
   "source": [
    "# 7) FD Solutions\n",
    "\n",
    "Last time:\n",
    "\n",
    "- Introduction to Numerical Analysis \n",
    "- Accuracy, Consistency, Stability, Convergence\n",
    "- Lax equivalence theorem\n",
    "\n",
    "Today:\n",
    "1. Measuring errors  \n",
    "2. Stability  \n",
    "3. Consistency  \n",
    "4. Second order derivatives  \n",
    "5. Matrix representations and properties  \n",
    "6. Second order derivative with Dirichelet boundary conditions  \n",
    "7. Discrete Green’s functions  \n",
    "8. Interpolation by Vandermonde matrices  \n",
    "9. High-order discretization of the Laplacian  \n",
    "10. Method of manufactured solutions\n",
    "11. Second order derivative with Neumann boundary conditions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be1e39",
   "metadata": {},
   "source": [
    "## 1.  Measuring errors\n",
    "\n",
    "Recap: \n",
    "* Last time we looked at the error of the _forward difference_, the _backward difference_, and the _centered difference_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4edd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LaTeXStrings\n",
    "default(linewidth=3)\n",
    "default(legendfontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83670652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are inline functions: take arrays x and u, spit out an array of differences u' over array x.\n",
    "diff1l(x, u) = x[2:end],   (u[2:end] - u[1:end-1]) ./ (x[2:end] - x[1:end-1])\n",
    "diff1r(x, u) = x[1:end-1], (u[2:end] - u[1:end-1]) ./ (x[2:end] - x[1:end-1])\n",
    "diff1c(x, u) = x[2:end-1], (u[3:end] - u[1:end-2]) ./ (x[3:end] - x[1:end-2])\n",
    "difflist = [diff1l, diff1r, diff1c]\n",
    "\n",
    "n = 40 \n",
    "h = 2 / (n - 1)\n",
    "x = LinRange(-3, 3, n)\n",
    "u = sin.(x)\n",
    "fig = plot(cos, xlims=(-3, 3), label = L\"f'(x)=\\cos(x)\")\n",
    "for d in difflist\n",
    "    xx, yy = d(x, u)\n",
    "    plot!(fig, xx, yy, marker=:circle, label=d)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba65f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5467f68",
   "metadata": {},
   "source": [
    "It's time for a worked exercise.\n",
    "* What does this code do?\n",
    "* What property of the difference formulas (methods) are we measuring? (hint: error vs N is ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "# We are measuring here the CONVERGENCE of these methods\n",
    "\n",
    "# How many gridpoints we use: 2^2, 2^3, ..., 2^10\n",
    "grids = 2 .^ (2:10)  # dot operator vectorizes, so \".^\" is just vectorized \"^\" over all entries of a collection/tuple/array\n",
    "# Grid spacing: 2^(-2), 2^(-3), ... 2^(-10)\n",
    "hs = 1 ./ grids \n",
    "\n",
    "# We define a function to measure the convergence of these methods\n",
    "# The func takes 3 arguments: `f` and `fprime` are callables (functions)\n",
    "# and `d` is an array containing numerical differences (derivatives)\n",
    "function refinement_error(f, fprime, d) \n",
    "    error = []\n",
    "    # Loop over the grids as they get more refined\n",
    "    for n in grids\n",
    "        x = LinRange(-3, 3, n) # domain\n",
    "        # Compute numerical derivative for this grid\n",
    "        xx, yy = d(x, f.(x))\n",
    "        # Compute error and add it to an array\n",
    "        push!(error, norm(yy - fprime.(xx), 2)/sqrt(n)) # push! = add to array; uses a normalized 2-norm, as Root Mean Square (RMS) \n",
    "    end\n",
    "    # Return the array of errors\n",
    "    error\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(xscale=:log10, yscale=:log10)\n",
    "for d in difflist\n",
    "    error = refinement_error(sin, cos, d)\n",
    "    plot!(fig, hs, error, marker=:circle, label=d)\n",
    "end\n",
    "plot!(fig, hs, [hs hs .^ 2], label=[\"h\" \"\\$h^2\\$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940d0c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ":::{note}\n",
    "- Different [norms](https://en.wikipedia.org/wiki/Norm_(mathematics)) can be defined. \n",
    "- The Euclidean norm  is also called the quadratic norm, $L^2$ norm,\n",
    "$ℓ^2$ norm (for vectors that have an infinite number of components, like sequences), $2$-norm, or square norm. This is a special case ($p=2$) of the $L^p$ norm for [$L^p$-spaces](https://en.wikipedia.org/wiki/Lp_space).\n",
    ":::\n",
    "\n",
    "- What happens if we use a 1-norm, 2-norm, or Inf-norm? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6dde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "function refinement_error(f, fprime, d) \n",
    "    error = []\n",
    "    for n in grids\n",
    "        x = LinRange(-3, 3, n)\n",
    "        xx, yy = d(x, f.(x))\n",
    "        push!(error, norm(yy - fprime.(xx), 1)) # uses a L-1 norm\n",
    "    end\n",
    "    error\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(xscale=:log10, yscale=:log10)\n",
    "for d in difflist\n",
    "    error = refinement_error(sin, cos, d)\n",
    "    plot!(fig, hs, error, marker=:circle, label=d)\n",
    "end\n",
    "plot!(fig, hs, [hs hs .^ 2], label=[\"h\" \"\\$h^2\\$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f044a8a",
   "metadata": {},
   "source": [
    "- The Root Mean Square (RMS)-like error used in the previous example measures the average size of the error _per grid point_.\n",
    "   * It is like asikng: \"On average, how wrong is my numerical derivative at a typical grid point?\" \n",
    "   * This question is useful, regardless of how coarse/fine the grid is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145b459",
   "metadata": {},
   "source": [
    "In general, the quantity \n",
    "\n",
    "$$\n",
    "\\|f\\|_h = \\left[ h \\sum_{m=-\\infty}^{\\infty} \\left| f_{m} \\right|^2\n",
    "\\right]^{1/2}\n",
    "$$\n",
    "\n",
    "is the $L^2$ norm of the grid function $f$, and is a measure of the size (energy) of the solution. \n",
    "- The multiplication by $h \\equiv \\Delta x$ is needed\n",
    "so that the norm is not sensitive to grid refinements (the number of\n",
    "points increases as $h\\rightarrow 0$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a6591",
   "metadata": {},
   "source": [
    "## 2. Stability\n",
    "\n",
    "\n",
    ":::{admonition} **Activity**\n",
    "- Read [**What is Numerical Stability?**](https://nhigham.com/2020/08/04/what-is-numerical-stability/) and discuss in small groups\n",
    "- Share insights in class discussion\n",
    ":::\n",
    "\n",
    "<img src=\"https://fncbook.com/build/backwarderror-55621e558c526e24b8fc1d61b00b65a3.svg\" width=\"90%\" />\n",
    "\n",
    "Source: [FNC: backward error](https://fncbook.com/stability/#backward-error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef969e3",
   "metadata": {},
   "source": [
    "### (Forward) Stability\n",
    "**\"nearly the right answer to nearly the right question\"**\n",
    "$$ \\frac{\\lvert \\tilde f(x) - f(\\tilde x) \\rvert}{| f(\\tilde x) |} \\in O(\\epsilon_{\\text{machine}}) $$\n",
    "for some $\\tilde x$ that is close to $x$. Recall that $\\tilde f$ is the computed, approximate solution and $f$ is the analytical one.\n",
    "\n",
    "### Backward Stability\n",
    "**\"exactly the right answer to nearly the right question\"**\n",
    "$$ \\tilde f(x) = f(\\tilde x) $$\n",
    "for some $\\tilde x$ that is close to $x$\n",
    "\n",
    "Note:\n",
    "* Every backward stable algorithm is ($\\implies$) stable.\n",
    "* Not every stable algorithm is backward stable.\n",
    "* The difference is in the _focus_: forward analysis is concerned with what the method reaches, while backward analysis looks at the problem being solved (which is why we can speak of ill-conditioned methods and ill-conditioned problems). \n",
    "* In a backward stable algorithm the errors introduced during the algorithm have the same effect as a small perturbation in the data. \n",
    "* If the backward error is the same size as any uncertainty in the data then the algorithm produces as good a result as we can expect.\n",
    "\n",
    "\n",
    "* An algorithm is backwards stable if for a small _input_ rel error you get a small _output_ rel error\n",
    "* = \"You get a nearly correct answer to the nearly correct question\"\n",
    "\n",
    "Question:\n",
    "* Are there \"rough\" functions for which these formulas estimate $u'(x_i) = 0$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c420954",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LinRange(-1, 1, 9)\n",
    "f_rough(x) = cos(.1 + 4π*x)\n",
    "fp_rough(x) = -4π*sin(.1 + 4π*x)\n",
    "\n",
    "plot(x, f_rough, marker=:circle, label = L\"f(x_i) =-4 \\pi \\sin(0.1 + 4 \\pi x_i)\") # Sparse sampling of the function\n",
    "plot!(f_rough, label = L\"f(x) =-4 \\pi \\sin(0.1 + 4 \\pi x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fe514",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(fp_rough, xlims=(-1, 1), label = L\"f'(x)=-16 \\pi^2 \\cos(0.1 + 4 \\pi x )\")\n",
    "for d in difflist\n",
    "    xx, yy = d(x, f_rough.(x))\n",
    "    plot!(fig, xx, yy, label=d, marker=:circle)\n",
    "end\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf7ec9",
   "metadata": {},
   "source": [
    "* If we have a solution $u(x)$, then $u(x) + f_{\\text{rough}}(x)$ is indistinguishable to our FD method.\n",
    "* Therefore, given a small input relative error, we can get a large output relative error.\n",
    "* There do not exist \"bad\" functions that also satisfy the equation.\n",
    "* The solution does not \"blow up\" for time-dependent problems.\n",
    "* Definition here is intentionally vague, and there are more subtle requirements for problems like incompressible flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27284b01",
   "metadata": {},
   "source": [
    "## 3. Consistency\n",
    "\n",
    "* When we apply the differential operator to the exact solution, we get a small residual.\n",
    "* The residual converges under grid refinement.\n",
    "* Hopefully fast as $h \\to 0$\n",
    "\n",
    "Recall, one part of **Lax equivalence theorem**: Consistency + Stability $\\implies$ Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c0fdf",
   "metadata": {},
   "source": [
    "## 4. Second order derivatives\n",
    "\n",
    "We can compute a second derivative by applying first derivatives twice, but which ones?\n",
    "\n",
    "### Example 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "function diff2a(x, u)\n",
    "    # Compute the second-derivative as a centered difference of a centered difference\n",
    "    xx, yy = diff1c(x, u)\n",
    "    diff1c(xx, yy)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function diff2b(x, u)\n",
    "    # Compute the second-derivative as a backward (left) difference of a forward (right) difference \n",
    "    xx, yy = diff1l(x, u)\n",
    "    diff1r(xx, yy) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diff2list = [diff2a, diff2b]\n",
    "n = 20\n",
    "x = LinRange(-3, 3, n)\n",
    "u = - cos.(x); # f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(cos, xlims=(-3, 3), label = L\"f''(x)=\\cos(x)\")\n",
    "for d2 in diff2list\n",
    "    xx, yy = d2(x, u)\n",
    "    plot!(fig, xx, yy, marker=:circle, label=d2)\n",
    "end\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca45eb2",
   "metadata": {},
   "source": [
    "### How fast do these approximations converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73634231",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = 2 .^ (3:10)\n",
    "hs = 1 ./ grids\n",
    "function refinement_error2(f, f_xx, d2)\n",
    "    error = []\n",
    "    for n in grids\n",
    "        x = LinRange(-3, 3, n)\n",
    "        xx, yy = d2(x, f.(x))\n",
    "        push!(error, norm(yy - f_xx.(xx), Inf\n",
    "        )) # which norm?\n",
    "    end\n",
    "    error\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(xlabel=\"h\", xscale=:log10, ylabel=\"Error\", yscale=:log10)\n",
    "for d2 in diff2list\n",
    "    error = refinement_error2(x -> -cos(x), cos, d2)\n",
    "    plot!(fig, hs, error, marker=:circle, label=d2)\n",
    "end\n",
    "plot!(fig, hs, hs .^ 2, label=\"\\$h^2\\$\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bea3ed",
   "metadata": {},
   "source": [
    "* Both methods are second order accurate.\n",
    "* The `diff2b` method is more accurate than `diff2a` (by a factor of 4)\n",
    "* The `diff2a` method can't compute derivatives at points adjacent the boundary.\n",
    "* We don't know yet whether either is stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c962c",
   "metadata": {},
   "source": [
    "## 5. Matrix representations and properties  \n",
    "\n",
    "* All our `diff*` functions thus far have been linear in `u`, therefore they can be represented as **differentiation matrices**.\n",
    "$$\\frac{u_{i+1} - u_i}{x_{i+1} - x_i} = \\begin{bmatrix} -1/h & 1/h \\end{bmatrix} \\begin{bmatrix} u_i \\\\ u_{i+1} \\end{bmatrix}$$\n",
    "\n",
    "* More generally: \n",
    "$$\\begin{bmatrix} u'(x_1) \\\\ u'(x_2) \\\\ \\vdots \\\\ u'(x_n) \\end{bmatrix} = \\begin{bmatrix} D_{11} & D_{12} & \\ldots & D_{1n} \\\\ D_{21} & D_{22} & \\ldots & D_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ D_{n1} & D_{2n} & \\ldots & D_{nn} \\end{bmatrix} \\begin{bmatrix} u(x_1) \\\\ u(x_2) \\\\ \\vdots \\\\ u(x_n) \\end{bmatrix}\\quad \\text{or} \\quad \\mathbf{u'} = \\mathbf{D} \\mathbf{u}$$\n",
    "\n",
    "### Example 7.2: First-derivative differentiation matrix on a uniform grid\n",
    "\n",
    "Let's see how the matrix form of the first-derivative operator can look like. \n",
    "\n",
    "Let us use a **second-order accurate centered scheme** for the interior points and **first-order one-sided** differences for the boundary points:\n",
    "\n",
    "- For the **interior points**, $x_2, \\ldots, x_{n-1}$, we have:\n",
    "\n",
    "$$\n",
    "u'(x_i) \\approx \\frac{u_{i+1} - u_{i-1}}{2 h}\n",
    "$$\n",
    "\n",
    "So the interior rows of the **matrix of coefficients** looks like:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2h} [-1, 0, 1]\n",
    "$$\n",
    "\n",
    "At the left boundary point, $x_1$, we **cannot** use the same centered difference formula\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{u_{2} - u_{0}}{2 h}\n",
    "$$\n",
    "\n",
    "because $ u_{0}$ is outside of the domain.\n",
    "\n",
    "Let's use a first-order accurate forward difference\n",
    "\n",
    "$$\n",
    "u'(x_1) \\approx \\frac{u_{2} - u_{1}}{h}\n",
    "$$\n",
    "\n",
    "For convenience, since interior point coefficients are scaled by $\\frac{1}{2h}$, we also want to scale this simlarly, therefore we multiply both the numerator and denomiantor by a factor of $2$:\n",
    "\n",
    "\n",
    "$$\n",
    "u'(x_1) \\approx \\frac{2u_{2} - 2u_{1}}{2h}\n",
    "$$\n",
    "\n",
    "Similarly, at the right boundary point, we **cannot** use the same centered difference formula\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{u_{n+1} - u_{n-1}}{2 h}\n",
    "$$\n",
    "\n",
    "because $ u_{n+1}$ is outside of the domain.\n",
    "\n",
    "Let's use a first-order accurate backard difference\n",
    "\n",
    "\n",
    "$$\n",
    "u'(x_n) \\approx \\frac{u_{n} - u_{n-1}}{h}\n",
    "$$\n",
    "\n",
    "and multiply this by a factor of $2$ to match the interior point coefficients that are scaled by $\\frac{1}{2h}$,\n",
    "\n",
    "$$\n",
    "u'(x_n) \\approx \\frac{2u_{n} - 2u_{n-1}}{2h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"First-derivative differentiation matrix on a uniform grid.\n",
    "\n",
    "- interior: 2nd-order centered\n",
    "- boundaries: 1st-order one-sided\n",
    "\"\"\"\n",
    "# Dfferentiation matrix for a first-derivative\n",
    "function diff1_mat(x)\n",
    "    n = length(x)\n",
    "    D = zeros(n, n)\n",
    "    h = x[2] - x[1]\n",
    "    D[1, 1:2] = [-1/h  1/h]              # Use a first-order forward difference at the left boundary\n",
    "    for i in 2:n-1\n",
    "        D[i, i-1:i+1] = [-1/2h  0  1/2h] # In the interior points, use a second-order centered difference\n",
    "    end\n",
    "    D[n, n-1:n] = [-1/h  1/h]            # Use a first-order backward difference at the right boundary\n",
    "    D\n",
    "end\n",
    "x = LinRange(-1, 1, 5) # with this grid, h = 1/2 => 2 h = 1\n",
    "diff1_mat(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ff6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "x = LinRange(-3, 3, n)\n",
    "plot(x, diff1_mat(x) * sin.(x), marker=:circle, label = L\"D * \\sin(x)\")\n",
    "plot!(cos, label = L\"f'(x)=\\cos(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f476d",
   "metadata": {},
   "source": [
    "### How accurate is this derivative matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(xscale=:log10, yscale=:log10, legend=:topleft)\n",
    "ref_error = refinement_error(sin, cos, (x, u) -> (x, diff1_mat(x) * u))\n",
    "plot!(fig, hs, ref_error, marker=:circle, label = \"refinement error\")\n",
    "plot!(fig, hs, hs, label=\"\\$h\\$\")\n",
    "plot!(fig, hs, hs .^ 2, label=\"\\$h^2\\$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f183b01",
   "metadata": {},
   "source": [
    "- But I thought we had chosen a second-order centered difference formula for the interior points? Then why do we get a lower order?\n",
    "\n",
    ":::{note}\n",
    "- In general, the approximation order of a problem is the _lowest_ order used in any part of the problem. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a0e0a",
   "metadata": {},
   "source": [
    "### Can we study it as a matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_spy(A)\n",
    "    cmax = norm(vec(A), Inf)\n",
    "    s = max(1, ceil(120 / size(A, 1)))\n",
    "    spy(A, marker=(:square, s), c=:diverging_rainbow_bgymr_45_85_c67_n256, clims=(-cmax, cmax))\n",
    "end\n",
    "\n",
    "D = diff1_mat(x)\n",
    "my_spy(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdvals(D) # Singular values given by the SVD decomposition, in dicreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bef1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond(D) # condition number of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888ddf2",
   "metadata": {},
   "source": [
    "### Condition number of a matrix\n",
    "\n",
    "The condition number of an _invertible_ matrix $A$ is given by\n",
    "\n",
    "$$ \\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert . $$\n",
    "\n",
    "If we think of the _action_ of a matrix on an input vector as performing the matrix-vector multiplication, we see that multiplying by a matrix is just as ill-conditioned of an operation as solving a linear system using that matrix.\n",
    "\n",
    "### SVD decomposition\n",
    "\n",
    "We have the following factorization\n",
    "$$ U \\Sigma V^T = A $$\n",
    "where $U$ and $V$ have orthonormal columns and $\\Sigma$ is diagonal with nonnegative entries.\n",
    "The entries of $\\Sigma$ are called [**singular values**](https://en.wikipedia.org/wiki/Singular_value) and this decomposition is the **singular value decomposition** (SVD).\n",
    "\n",
    "**Singular values** of a linear operator (matrix $A$) are the square roots of the (necessarily non-negative) eigenvalues of the self-adjoint operator $A^{*}A$ (where $A^{*}$ denotes the [adjoint](https://en.wikipedia.org/wiki/Hermitian_adjoint) of $A$.)\n",
    " \n",
    "It may remind you of an eigenvalue decomposition $X \\Lambda X^{-1} = A$, but\n",
    "* the SVD exists for all matrices (including non-square and deficient matrices)\n",
    "* $U,V$ have orthogonal columns (while $X$ can be arbitrarily ill-conditioned).\n",
    "* Indeed, if a matrix is symmetric and positive definite (all positive eigenvalues), then $U=X$ and $\\Sigma = \\Lambda$.\n",
    "* If we think of orthogonal matrices as reflections/rotations, this says any matrix can be represented by the sequence of operations: reflect/rotate, diagonally scale, and reflect/rotate again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ce56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"../img/svd_viz.svg\") do f\n",
    "   display(\"image/svg+xml\", read(f, String))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96154d18",
   "metadata": {},
   "source": [
    "### Condition number via SVD\n",
    "\n",
    "$$ \\kappa(A) = \\lVert A \\rVert \\ \\lVert A^{-1} \\rVert $$\n",
    "\n",
    "Or, in terms of the SVD\n",
    "\n",
    "$$ U \\Sigma V^T = \\texttt{svd}(A) $$\n",
    "where\n",
    "$$ \\Sigma = \\begin{bmatrix} \\sigma_{\\max} && \\\\ & \\ddots & \\\\ && \\sigma_{\\min} \\end{bmatrix}, $$\n",
    "\n",
    "where, $(\\sigma_{1}, \\sigma_{2}, \\ldots)$ are the non-negative real numbers called **singular values** of $A$ (usually listed in decreasing order).\n",
    "\n",
    "We have that the matrix condition number is given by:\n",
    "\n",
    "$$ \\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\texttt{cond}(A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844503d",
   "metadata": {},
   "source": [
    "## 6. Second-derivative with Dirichlet boundary conditions\n",
    "\n",
    "Recall the Poisson equation with non-homogeneous Dirichelet boundary conditions: \n",
    "\n",
    "\\begin{gather} -\\frac{d^2 u}{dx^2} = f(x) \\quad x \\in \\Omega = (-1,1) \\\\\n",
    "u(-1) = a, \\quad u(1) = b .\n",
    "\\end{gather}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed35e16",
   "metadata": {},
   "source": [
    "* Turn this into a linear system by replacing \n",
    "    * $x \\to [x_1, x_2, \\ldots, x_n] = \\mathbf{x}$,\n",
    "    * $u(x) \\to [u_1, u_2, \\ldots, u_n] = \\mathbf{u}$,\n",
    "    * $f(x) \\to [f_1, f_2, \\ldots, f_n] = \\mathbf{f}$,\n",
    "    * $\\frac{d^2}{dx^2} \\to \\mathbf{D}^2$.\n",
    "    \n",
    "    $$ \\mathbf{D}^2\\mathbf{u} = \\mathbf{f}$$\n",
    "    \n",
    "* How to encode left boundary condition? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353ff71",
   "metadata": {},
   "source": [
    "The left endpoint in our example boundary value problem has a Dirichlet boundary condition,\n",
    "$$u(-1) = a . $$\n",
    "With finite difference methods, we have an explicit degree of freedom $u_1 = u(x_1 = -1)$ at that endpoint.\n",
    "When building a matrix system for the BVP, we can implement this boundary condition by modifying the first row of the matrix,\n",
    "$$ \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ \\\\ & & A_{2:n,:} & & \\\\ \\\\ \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ \\\\ u_{2:n} \\\\ \\\\ \\end{bmatrix} = \\begin{bmatrix} a \\\\ \\\\ f_{2:n} \\\\ \\\\ \\end{bmatrix} . $$\n",
    "\n",
    "* This matrix is now not symmetric even if the **interior** part $A_{2:n-1} ( = D^2)$ is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function laplacian_dirichlet(x)\n",
    "    n = length(x)\n",
    "    D = zeros(n, n)\n",
    "    h = x[2] - x[1]\n",
    "    D[1, 1] = 1\n",
    "    for i in 2:n-1\n",
    "        D[i, i-1:i+1] = (1/h^2) * [-1, 2, -1]\n",
    "    end\n",
    "    D[n, n] = 1\n",
    "    D\n",
    "end\n",
    "\n",
    "x = LinRange(-1, 1, 5) # with this grid, h = 1/2 => h^2 = 1/4 => 1/h^2 = 4\n",
    "laplacian_dirichlet(x)[2:end-1,:] # show only the interior rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db10d0",
   "metadata": {},
   "source": [
    "###  Laplacian operator as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = laplacian_dirichlet(x)\n",
    "my_spy(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdvals(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b32516",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60073ea7",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "* For now, the right boundary condition is also Dirichlet (we will consider Neumann later), $u(1) = b$.\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ \\\\ & & A_{2:n-1,:} & & \\\\ \\\\ \\\\ 0&0 &0 &0 &0 & 1 \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ \\\\ u_{2:n-1} \\\\ \\\\ \\\\ u_n \\end{bmatrix} = \\begin{bmatrix} a \\\\ \\\\ f_{2:n-1} \\\\ \\\\ \\\\ b \\end{bmatrix} . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63157f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L = laplacian_dirichlet(x)\n",
    "f = one.(x) # constant forcing\n",
    "f[1] = 0 # enforcing the left BC\n",
    "f[end] = 0; # enforcing the right BC\n",
    "plot(x, f, label = L\"f(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = L \\ f # This is syntax for the direct solution of the system Lu = f (similar to MATLAB syntax)\n",
    "plot(x, u, label = L\"u = L^{-1} f \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811d5ae",
   "metadata": {},
   "source": [
    "## 7. Discrete \"Green's functions\"\n",
    "\n",
    "### Review: Matrix inverses\n",
    "Before diving into discrete \"Green's functions\", let's review matrix inverses from Linear Algebra. \n",
    "\n",
    "The inverse $A^{−1}$ of a matrix $A$ is such that $A A^{-1} = I$. Let’s think about what this means. Each column j of $A^{−1}$ has a special meaning: $A A^{-1} = I$, so \n",
    "$$A \\cdot (\\textrm{column }j \\textrm{ of }A^{-1}) = (\\textrm{column }j \\textrm{ of } I) = \\mathbf{e}_j,$$\n",
    "\n",
    "where $\\mathbf{e}_j$ is the unit vector $(0,0,\\ldots,0,1,0,\\ldots, 0)^T$ which has a $1$ in the $j$-th row. \n",
    "\n",
    "Multiplying both sides of the the expression above by $A^{-1}$, \n",
    "we obtain \n",
    "\n",
    "$$ A^{-1} A \\cdot (\\textrm{column }j \\textrm{ of }A^{-1}) = A^{-1} \\mathbf{e}_j ,$$\n",
    "\n",
    "hence,\n",
    "\n",
    "$$(\\textrm{column } j \\textrm{ of } A^{-1}) = A^{-1}\\mathbf{e}_j,$$\n",
    "\n",
    "which looks like the solution to $A\\mathbf{u} = \\mathbf{f}$ with  $\\mathbf{e}_j$ on theright-hand-side.\n",
    "\n",
    "If we are solving $A\\mathbf{u} = \\mathbf{f}$, the statement $\\mathbf{u} = A^{-1}\\mathbf{f}$ can be written as a superposition of columns of $A^{-1}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = A^{-1}\\mathbf{f} = \\sum_j (\\textrm{column }j \\textrm{ of }A^{-1}) f_j = A^{-1} \\sum_j f_j \\mathbf{e}_j .\n",
    "$$\n",
    "\n",
    "Since $\\sum_j f_j \\mathbf{e}_j$ is just writing $ \\mathbf{f}$ in the canonical basis of the $\\mathbf{e}_j$'s, we can interpret $A^{-1} \\mathbf{f}$ as _writing_ $\\mathbf{f}$ in the canonical basis of the $\\mathbf{e}_j$ unit vectors and then _summing_ (_superimposing_) the solutions for each $\\mathbf{e}_j$. \n",
    "\n",
    "Going one step further, we can write \n",
    "\n",
    "$$\n",
    "u_i = \\left( A^{-1} \\mathbf{f} \\right)_i = \\sum_j  \\left( A^{-1} \\right)_{i,j}f_j .\n",
    "$$\n",
    "\n",
    "We can now have a simple \"physical\" interpretation of the matrix elements of $A^{-1} $:\n",
    "\n",
    "- $\\left( A^{-1} \\right)_{i,j}$ is the \"**effect**\" (solution) at $i$ of a \"**source**\" at $j$ (a RHS $\\mathbf{e}_j$) [and  column $j$ of $ A^{-1} $ is the effect (solution) _everywhere_  from the source at $j$].\n",
    "\n",
    "### Laplacian matrix\n",
    "\n",
    "Recall that one interpretation of the Poisson's problem\n",
    "\n",
    "$$\n",
    "\\nabla^2  \\mathbf{u} = \\mathbf{f},\n",
    "$$\n",
    "\n",
    "is that $\\mathbf{u}$ is a displacement of a stretched string (in 1D) or membrane (in 2D) and that $f$ (or $-f$, if we followed the physics notation) is a force density (pressure). Then we can see:\n",
    "\n",
    "* Green's functions as impulse response\n",
    "* If we write the LHS of our equation as a linear differential operator $L$ acting on $u$,\n",
    "\n",
    "$$ Lu(x) = f(x),$$\n",
    "\n",
    "then the Green's function $G(x, s)$ ($s$ is for \"source\") is defined as\n",
    "\n",
    "$$ LG(x, s) = \\delta(x - s) $$\n",
    "\n",
    "* Motivation: once we have $G(x, s)$, we can \"build up\" solutions from it for different $f$, by superposition.\n",
    "\n",
    "Integrating the definition of the Green's function, we get\n",
    "\n",
    "$$ \\int LG(x,s)\\,f(s)\\,ds=\\int \\delta (x-s)\\,f(s)\\,ds=f(x)\\, .$$\n",
    "\n",
    "Due to $L$ only acting on $x$ and being linear, we can bring $L$ out:\n",
    "\n",
    "$$ L\\left(\\int G(x,s)\\,f(s)\\,ds\\right)=f(x)\\, ,$$\n",
    "meaning\n",
    "\n",
    "$$ u(x)=\\int G(x,s)\\,f(s)\\,ds. $$\n",
    "\n",
    "* Position of the source, $s$, can only be one of the $x_i$ in the linear system\n",
    "* In matrix form:\n",
    "\n",
    "$$ LG = I $$\n",
    "\n",
    "* So $G = L^{-1}$, and the columns of $L^{-1}$ are the discrete \"Green's functions\" for varying $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, inv(L)[:, 2], label = L\"L^{-1}_{:,2}\") # second column of L^{-1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc41aa59",
   "metadata": {},
   "source": [
    "We can see that the columns of $L^{-1}$ in 1D are very much like what you might intuitively expect if you pulled on a string at \"one point.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e276fb",
   "metadata": {},
   "source": [
    "### Discrete eigenfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1daf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LinRange(-1, 1, 10)\n",
    "L = laplacian_dirichlet(x)\n",
    "Lambda, V = eigen(L) # returns an eigen factorization (eigenvalues, matrix of eigenvectors)\n",
    "plot(Lambda, marker=:circle, label = L\"\\lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72eb52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, V[:, 1:4]) # first 4 eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599fcf6",
   "metadata": {},
   "source": [
    "### Outlook on our method\n",
    "\n",
    "#### Pros\n",
    "* Consistent\n",
    "* Stable\n",
    "* Second order accurate (we hope)\n",
    "\n",
    "#### Cons\n",
    "* Only second order accurate (at best)\n",
    "* Worse than second order on non-uniform grids\n",
    "* Worse than second order at Neumann boundaries\n",
    "* Boundary conditions break symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870dc4f",
   "metadata": {},
   "source": [
    "## 8. Interpolation by Vandermonde matrices\n",
    "\n",
    "* Let's say we solved our IBVP with a FD difference method. Now we have $\\mathbf{u} = [u(x_1), u(x_2), \\ldots, u(x_n) ] := [ u_1, u_2, \\ldots, u_n]$\n",
    "\n",
    "* How do we get $u(x)$ from the vector $\\mathbf{u}$?\n",
    "\n",
    "* Try fitting a polynomial that goes through the \"data\" $u_1$, $u_2$, $\\ldots$, of the form\n",
    "\n",
    "\n",
    "\n",
    "$$ p(x) = c_0 + c_1 x + c_2 x^2 + \\dotsb $$\n",
    "\n",
    "that assumes function values $p(x_i) = u_i$.\n",
    "\n",
    "### Questions\n",
    "\n",
    "* What degree does the polynomial need to be (how many $c_i$-s) and why?\n",
    "* What constraints can we write down for the $c_i$-s?\n",
    "* What _linear system_ can we write the constraints as?\n",
    "\n",
    "### Answers\n",
    "\n",
    "* Polynomial needs to be degree $n - 1$.\n",
    "\n",
    "* Constraints:\n",
    "\n",
    "$$ p(x_1) = c_0 + c_1 x_1 + c_2 x_1^2 + \\dotsb = u(x_1) $$\n",
    "\n",
    "$$ p(x_2) = c_0 + c_1 x_2 + c_2 x_2^2 + \\dotsb = u(x_2) $$\n",
    "\n",
    "$$ \\vdots $$\n",
    "\n",
    "* We can write them as a linear system called a **Vandermonde** matrix:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix} 1 & x_1 & x_2^2 & \\dotsb \\\\\n",
    "    1 & x_2 & x_3^2 & \\dotsb \\\\\n",
    "    1 & x_3 & x_3^2 & \\dotsb \\\\\n",
    "    \\vdots & & & \\ddots \\end{bmatrix}}_V \\begin{bmatrix} c_0 \\\\ c_1 \\\\ c_2 \\\\ \\vdots \\end{bmatrix} = \\begin{bmatrix} u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\end{bmatrix} \\\\\n",
    "    V\\mathbf{c} = \\mathbf{u} .    \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4051143",
   "metadata": {},
   "outputs": [],
   "source": [
    "function vander(x, k=nothing)\n",
    "    if k === nothing\n",
    "        k = length(x)\n",
    "    end\n",
    "    V = ones(length(x), k)\n",
    "    for j = 2:k\n",
    "        V[:, j] = V[:, j-1] .* x\n",
    "    end\n",
    "    V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vander(LinRange(-1, 1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond(vander(LinRange(-1, 1, 5))) # condition number "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1d018",
   "metadata": {},
   "source": [
    "* The condition number of the Vandermonde matrix can get as large as $10^{16}$ ($\\approx 1/\\varepsilon_M$ in general) before we start losing digits\n",
    "* Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27471ecc",
   "metadata": {},
   "source": [
    "###  Fitting a polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b84ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "x = LinRange(-2, 2, k)\n",
    "u = sin.(x)\n",
    "V = vander(x)\n",
    "c = V \\ u # c = V^{-1}u\n",
    "scatter(x, u, label=\"\\$u_i = sin (x_i)\\$\", legend=:topleft)\n",
    "plot!(x -> (vander(x, k) * c)[1,1], label=\"\\$p(x)\\$\")\n",
    "plot!(sin, label=L\"\\sin(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52718813",
   "metadata": {},
   "source": [
    "### Differentiating\n",
    "\n",
    "We're given the coefficients $c = V^{-1} u$ of the polynomial\n",
    "$$\n",
    "p(x) = c_0 + c_1 x + c_2 x^2 + \\dotsb.\n",
    "$$\n",
    "\n",
    "What is\n",
    "\n",
    "\\begin{align} p(0) &= c_0 \\\\\n",
    "p'(0) &= c_1 \\\\ \n",
    "p''(0) &= c_2 \\cdot 2\\\\\n",
    "p^{(k)}(0) &= c_k \\cdot k! .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function fdstencil1(source, target)\n",
    "    \"first derivative stencil from source to target\"\n",
    "    x = source .- target\n",
    "    V = vander(x)\n",
    "    inv(V)[2, :]' # coefficients for the first derivative, as a row vector of of V^{-1}\n",
    "end\n",
    "plot([z -> fdstencil1(x, z) * u, cos], label=[\"fdstencil1\" L\"\\cos(x)\"], xlims=(-3,3))\n",
    "scatter!(x, 0*x, label=\"grid points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec84217",
   "metadata": {},
   "source": [
    "### Arbitrary order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function fdstencil(source, target, k)\n",
    "    \"kth derivative stencil from source to target\"\n",
    "    x = source .- target\n",
    "    V = vander(x)\n",
    "    rhs = zero(x)'\n",
    "    rhs[k+1] = factorial(k)\n",
    "    rhs / V # rhs * inv(V), without explicitly calling inv\n",
    "end\n",
    "fdstencil(x, 0.5, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ea223",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([z -> fdstencil(x, z, 2) * u,\n",
    "        z -> -sin(z)],  label=[\"fdstencil\" L\"-\\sin(x)\"], xlim=(-3, 3)) \n",
    "scatter!(x, 0*x, label=\"grid points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a203bd8",
   "metadata": {},
   "source": [
    "We didn't call `inv(V)`; what's up?\n",
    "\n",
    "Recall that we have\n",
    "\n",
    "$$\n",
    "V c = u ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "c = V^{-1}u.\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(0) = c_0 + c_1 0 + c_2 0 + \\dotsb  = e_0^T \\underbrace{V^{-1} u}_c = \\underbrace{e_0^T V^{-1}}_{s^0} u\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "p(0) = s^0 u\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "> Evaluating the interpolating polynomial at $0$ is just a weighted sum of the data values. And those weights are the first row of $V^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d852c",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "| Row of $V^{-1}$ | Meaning |\n",
    "|-------------------|---------|\n",
    "| Row 1             | Interpolation weights for evaluating at $0$ |\n",
    "| Row 2             | Weights for first derivative at $0$|\n",
    "| Row 3             | Weights for second derivative at $0$ |\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74d52b",
   "metadata": {},
   "source": [
    "### Exercise 7.3: Convergence order \n",
    "\n",
    "* Looking at this convergence graph, deduce the order of convergence for our stencil (for differentiating twice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85656dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = 2 .^ -LinRange(-4, 10, 10)\n",
    "function diff_error(u, du, h; n, k, z=0)\n",
    "    x = LinRange(-h, h, n) .+ .5\n",
    "    fdstencil(x, z, k) * u.(x) - du.(z)\n",
    "end\n",
    "errors = [diff_error(sin, t -> -sin(t), h, n=5, k=2, z=.5+0.1*h)\n",
    "    for h in hs]\n",
    "plot(hs, abs.(errors), marker=:circle)\n",
    "\n",
    "plot!(h -> h^3, label=\"\\$h^?\\$\", xscale=:log10, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164fff41",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* When using $n=3$ points, we fit a polynomial of degree 2 and have error $O(h^3)$ for interpolation $p(0)$.\n",
    "* Each derivative gives up one order of accuracy in general.\n",
    "* For a finite-difference stencil built by exactness on polynomials up to degree $n-1$, the error for the $k$-th derivative behaves like $O(h^{n-k})$.\n",
    "    - In our Exercise 7.3, $n=5$ (five points) and $k=2$ (second derivative), so $n-k = 3$, i.e, third-order convergence.\n",
    "* Centered diff on uniform grids can have extra cancellation (_superconvergence_).\n",
    "* The Vandermonde matrix is notoriously **ill-conditioned** with many points $n$. The recommendation is to use a [stable algorithm from Fornberg](https://epubs.siam.org/doi/10.1137/S0036144596322507).\n",
    "* The real trouble arises when the condition number is $\\kappa \\approx 1/\\varepsilon_M$ (see this recent [preprint](https://arxiv.org/abs/2212.10519)).\n",
    "* Question: what could we have changed about the interpolation method? Hint: there are _two_ main degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777eff30",
   "metadata": {},
   "source": [
    "Answers: \n",
    " * We could have changed the representation: $p(x) = c_0 + c_1 x + c_2 x^2 + \\ldots$ uses the _monomial basis_ $1, x, x^2, \\ldots$, but we could have used a different _basis set_ of functions. This in turn changes what the unknowns $c_0, c_1, \\ldots$ represent.\n",
    "  * We also need to be careful in picking the nodes $x_1, x_2, \\ldots$, on which the entries of the Vandermonde matrix explicitly depend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4149a5",
   "metadata": {},
   "source": [
    "## 9. High-order discretization of the Laplacian  \n",
    "\n",
    "Now that we have developed an algorithm for the arbitrary $k$-th derivative stencil from a given source to target, we can use it for any high-order FD discretization of the Laplacian.\n",
    "\n",
    "For the Poisson problem $-u_{xx} = f$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Poisson solver with arbitrary boundary conditions. In the interior, it uses fdstencil(..., 2) for the second-derivative.\n",
    "\n",
    "The BCs are specified as:\n",
    "\n",
    "left = (k, value)\n",
    "and\n",
    "right = (m, value)\n",
    "\n",
    "Examples:\n",
    "left = (0, 0) → enforce u(x_1)=0\n",
    "left = (1, 0) → enforce u'(x_1)=0\n",
    "left = (2, 0) → enforce u''(x_1)=0, and so on\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Solver for Poisson problem with arbitrary order\n",
    "function poisson(x, spoints, forcing; left=(0, zero), right=(0, zero)) # keyword arguments (optional); default homogeneous Dirichelet BCs\n",
    "    n = length(x)\n",
    "    L = zeros(n, n)\n",
    "    rhs = forcing.(x) # forcing is an input function\n",
    "    for i in 2:n-1\n",
    "        jleft = min(max(1, i-spoints÷2), n-spoints+1)\n",
    "        js = jleft : jleft + spoints - 1\n",
    "        L[i, js] = -fdstencil(x[js], x[i], 2) # Second derivative via the stencil we derived\n",
    "    end\n",
    "    L[1,1:spoints] = fdstencil(x[1:spoints], x[1], left[1]) # Exercise: what do these two lines mean? fdstencil returns a row vector of coeff [c_1, c_2, ..., c_spoints] s.t. c_1 u_1 + c_2 u_2 + ... = u^(k) (x_1)\n",
    "    L[n,n-spoints+1:n] = fdstencil(x[n-spoints+1:n], x[n], right[1])  # x[n-spoints+1:n] are the last spoints grid points\n",
    "    rhs[1] = left[2](x[1]) # second-derivative condition\n",
    "    rhs[n] = right[2](x[n]) # second-derivative condition\n",
    "    L, rhs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf56f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, b = poisson(LinRange(-1, 1, 6), 3, zero, left=(1, zero)) # Nodes are 6 equispaced points on [-1, 1]\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e971d50",
   "metadata": {},
   "source": [
    "* **Question:** how do we test the convergence of this PDE solver (and the correctness of the code)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1856a3",
   "metadata": {},
   "source": [
    "* Good practice (test-driven design):\n",
    "    * Think of what a function will do (inputs, outputs, its objective)\n",
    "    * Write its docstring / documentation\n",
    "    * Think of a test for its functionality\n",
    "    * Write a _unit test_ \n",
    "    * Then, and only then, write the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7c5ae",
   "metadata": {},
   "source": [
    "## 10. Method of manufactured solutions\n",
    "= A way to validate that a PDE system is being solved correctly. To verify our numerics.\n",
    "\n",
    "### Problem: analytic solutions to PDEs are hard to find\n",
    "\n",
    "Let's choose a smooth function with rich derivatives,\n",
    "$$ u(x) = \\tanh(x) . $$\n",
    "Then $$ u'(x) = \\cosh^{-2}(x) $$ and $$ u''(x) = -2 \\tanh(x) \\cosh^{-2}(x) . $$\n",
    "\n",
    "* This works for nonlinear too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LinRange(-2, 2, 21)\n",
    "L, rhs = poisson(x, 5,\n",
    "    x -> 2 * tanh(x) / cosh(x)^2,\n",
    "    left=(0, tanh), # left homogeneous Dirichelet BC\n",
    "    right=(1, x -> cosh(x)^-2)) # right homogeneous Neumann BC\n",
    "u = L \\ rhs\n",
    "plot(x, u, marker=:circle, legend=:topleft, label = \"discrete\")\n",
    "plot!(tanh, label = L\"\\tanh(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf6fc7",
   "metadata": {},
   "source": [
    "### Convergence rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877705a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 2 .^ (4:10)\n",
    "hs = 1 ./ ns\n",
    "function poisson_error(n; spoints=3)\n",
    "    x = LinRange(-2, 2, n)\n",
    "    L, rhs = poisson(x, spoints, x -> 2 * tanh(x) / cosh(x)^2,\n",
    "        left = (0, tanh),\n",
    "        right = (1, x -> cosh(x)^-2))\n",
    "    u = L \\ rhs\n",
    "    norm(u - tanh.(x), Inf)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d81af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hs, [poisson_error(n, spoints=9) for n in ns], marker=:circle, label = \"error\")\n",
    "plot!(h -> h^8, label=\"\\$h^8\\$\", xscale=:log10, yscale=:log10)\n",
    "\n",
    "# Question: what's going wrong at small h?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479050c",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- We saw above that, with $n$ points, with this general Vandermonde-based stencil generator, the error for the $k$-th derivative behaves like $O(h^{n-k})$. Here we are approximating a Poisson (Laplacian) operator, so second-derivative $u''$, with $9$ points, hence $O(h^7)$.\n",
    "- **But**: for symmetric centered stencils on a uniform grid, odd powers cancel out, hence, the leading error term becomes an even power. So instead of $O(h^7)$, we get _superconvergence_ $O(h^8)$. (This is a symmetry effect).\n",
    "- Why doesn't the treatment of the boundary conditions \"destroy\" or \"pollute\" the order of convergence globally now? \n",
    "  * Because the boundary conditions are built with the same number of points (`spoints`), so they are also high-order accurate. Hence, the global system retains the high-order accuracy.\n",
    "-  What's going on for small values of $h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ea39e",
   "metadata": {},
   "source": [
    "## 11. Second order derivative with Neumann boundary conditions  \n",
    "\n",
    "We have seen how to implement Dirichelet boundary conditions. Let's revisit that from the point of view of symmetry of the resulting differentiation matrix.\n",
    "\n",
    "### Symmetry in boundary conditions: Dirichlet\n",
    "\n",
    "We have implemented Dirichlet conditions by modifying the first row of the matrix,\n",
    "$$ \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ \\\\ & & A_{2:n,:} & & \\\\ \\\\ \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ \\\\ u_{2:n} \\\\ \\\\ \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ \\\\ f_{2:n} \\\\ \\\\ \\end{bmatrix} . $$\n",
    "\n",
    "* This matrix is not symmetric even if $A_{2:n,:}$ is.\n",
    "* We can eliminate $u_1$ and create a reduced system for $u_{2:n}$.\n",
    "* Generalize: consider a $2\\times 2$ block system\n",
    "$$ \\begin{bmatrix} I & 0 \\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix} .$$\n",
    "\n",
    "We can rearrange as\n",
    "$$ A_{22} u_2 = f_2 - A_{21} f_1, $$\n",
    "which is symmetric if $A_{22}$ is.\n",
    "* This is called \"lifting\" and is often done implicitly in the mathematics literature. It is convenient for linear solvers and eigenvalue solvers, but inconvenient for I/O and postprocessing, as well as some nonlinear problems.\n",
    "* Convenient alternative: write\n",
    "$$ \\begin{bmatrix} I & 0 \\\\ 0 & A_{22} \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 - A_{21} f_1 \\end{bmatrix}, $$\n",
    "which is symmetric and decouples the degrees of freedom associated with the boundary. This method applies cleanly to nonlinear problems.\n",
    "* Optionally scale the identity by some scalar related to the norm of $A_{22}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7a370",
   "metadata": {},
   "source": [
    "### Symmetry in boundary conditions: Neumann\n",
    "\n",
    "\n",
    "Let's revisit the Poisson equation, now with mixed Dirichelet and Neumann boundary conditions: \n",
    "\n",
    "\\begin{gather} -\\frac{d^2 u}{dx^2} = f(x) \\quad x \\in \\Omega = (-1,1) \\\\\n",
    "u(-1) = a, \\quad \\frac{d u}{d x}(1) = b .\n",
    "\\end{gather}\n",
    "\n",
    "\n",
    "Consider a FD discretization of the Neumann boundary condition on the right boundary:\n",
    "$$ \\frac{du}{dx}(1) = b . $$\n",
    "\n",
    "1. We could use a one-sided backward difference formula as in\n",
    "$$ \\frac{u_n - u_{n-1}}{h} = b . $$\n",
    "  * this gives us an extra discretization choice\n",
    "  * may reduce order of accuracy compared to interior discretization, and we lose symmetry.\n",
    "2. Temporarily introduce a **ghost point** value $u_{n+1} = u(x_{n+1} = 1 + h)$ (possibly more) and define it to be a reflection of the values from inside the domain. In the case of a homogeneous Neumann boundary condition, $b=0$, this reflection is $u_{n+1} = u_{n-1}$. More generally,\n",
    "\n",
    "$$\n",
    "\\frac{u_{n+1} - u_{n-1}}{2 h} = b,\n",
    "$$\n",
    "\n",
    "$$ u_{n+1} = u_{n-1} + 2b h . $$\n",
    "\n",
    "With this definition of ghost values, we can apply the interior discretization at the boundary. For our reference equation, we would write\n",
    "\n",
    "$$ \\frac{-u_{n-1} + 2 u_n - u_{n+1}}{h^2} = f(x_n) $$\n",
    "\n",
    "which simplifies to $$ \\frac{u_n - u_{n-1}}{h^2} = f(x_n)/2 + b/h $$\n",
    "after dividing by 2 and moving the boundary term to the right hand side."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
